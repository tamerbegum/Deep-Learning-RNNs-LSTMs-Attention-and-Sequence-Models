{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXAE3DipNM3B"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# EXERCISE 3\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9VE_5_NL0DJ",
        "outputId": "632cdd8a-9c28-4e32-ec64-dd1dc459749b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Implemented Basic RNN Cell.\n",
            "Implemented LSTM Cell.\n",
            "Implemented Scaled Dot-Product Attention (Self-Attention basis).\n",
            "Implemented Seq2Seq Model with Attention.\n",
            "\n",
            "--- 5.1 Bidirectionality Example ---\n",
            "Bidirectional LSTM Output shape: torch.Size([4, 10, 512])\n",
            "Bidirectional LSTM Final Hidden shape: torch.Size([4, 4, 256])\n",
            "Bidirectional LSTM Final Cell shape: torch.Size([4, 4, 256])\n",
            "\n",
            "--- 5.2 GRU Cell Implementation ---\n",
            "Implemented GRU Cell.\n",
            "\n",
            "--- 5.3 Masking Examples ---\n",
            "Created Attention Mask: tensor([[[[1., 1., 1., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1., 1., 1., 1.]]]])\n",
            "Attention Mask shape: torch.Size([2, 1, 1, 5])\n",
            "Loss before masking (shape torch.Size([2, 5])):\n",
            "tensor([[7.4727, 4.1645, 5.3687, 4.9885, 3.9675],\n",
            "        [4.9246, 5.6417, 3.6404, 5.4563, 4.7447]])\n",
            "Loss after masking:\n",
            "tensor([[7.4727, 4.1645, 5.3687, 0.0000, 0.0000],\n",
            "        [4.9246, 5.6417, 3.6404, 5.4563, 4.7447]])\n",
            "Mean masked loss: 5.1766815185546875\n",
            "\n",
            "--- 5.4 Multi-Head Attention Implementation ---\n",
            "Implemented Multi-Head Attention structure.\n"
          ]
        }
      ],
      "source": [
        "# import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. Basic RNN Cell Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "class SimpleRNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(SimpleRNNCell, self).__init__()\n",
        "        # Store sizes for clarity and potential use later.\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define learnable weights for the transformations.\n",
        "        # nn.Parameter registers these tensors as model parameters, so they are tracked by optimizers.\n",
        "        # weight_ih: Transforms the input 'x_t' to the hidden space.\n",
        "        # Shape: (hidden_size, input_size)\n",
        "        self.weight_ih = nn.Parameter(torch.randn(hidden_size, input_size))\n",
        "        # weight_hh: Transforms the previous hidden state 'h_{t-1}' to the hidden space.\n",
        "        # Shape: (hidden_size, hidden_size)\n",
        "        self.weight_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        # bias: Added after the linear transformations. Improves model flexibility.\n",
        "        # Shape: (hidden_size)\n",
        "        self.bias = nn.Parameter(torch.randn(hidden_size))\n",
        "\n",
        "        # Initialize weights randomly (randn). Used torch.randn for simplicity—easier to implement in PyTorch.\n",
        "        # Other initializations like Xavier or Kaiming could be used for potentially better convergence.\n",
        "        # torch.randn(...) initializes weights with standard normal distribution. It's simple but can lead to exploding/vanishing gradients in deeper nets.\n",
        "        # nn.init.xavier_uniform_ (Xavier/Glorot): better for tanh or sigmoid activations.\n",
        "        # nn.init.kaiming_uniform_ (He): better for ReLU.\n",
        "        # https://paperswithcode.com/method/xavier-initialization\n",
        "        # https://paperswithcode.com/method/he-initialization\n",
        "    def forward(self, input_tensor, hidden_state):\n",
        "        # Use F.linear for matrix multiplication and bias addition.\n",
        "        # It's equivalent to 'input @ weight.T + bias'.\n",
        "        # Calculate the linear transformation of the input.\n",
        "        input_transform = F.linear(input_tensor, self.weight_ih)\n",
        "        # Calculate the linear transformation of the previous hidden state and add bias.\n",
        "        hidden_transform = F.linear(hidden_state, self.weight_hh, self.bias)\n",
        "\n",
        "        # Apply the hyperbolic tangent (tanh) activation function.\n",
        "        # This introduces non-linearity and keeps the hidden state values\n",
        "        # bounded between -1 and 1, helping to prevent exploding gradients.\n",
        "        # Compute the next hidden state: h_t = tanh(W_ih * x_t + W_hh * h_{t-1} + b)\n",
        "        h_next = torch.tanh(input_transform + hidden_transform)\n",
        "\n",
        "        return h_next\n",
        "\n",
        "# --- Annotations ---\n",
        "# • The `SimpleRNNCell` class defines the computation for a single time step (t).\n",
        "# • `__init__`: Sets up the necessary learnable parameters (weights `weight_ih`, `weight_hh`, and `bias`). These parameters will be adjusted during training.\n",
        "# • `weight_ih`: Learns to map the input features (`input_size`) to the hidden state dimension (`hidden_size`).\n",
        "# • `weight_hh`: Learns to map the previous hidden state (`hidden_size`) to the current hidden state dimension (`hidden_size`), capturing temporal dependencies.\n",
        "# • `bias`: An additive term to shift the activation function's input, increasing model capacity.\n",
        "# • `forward`: Implements the core RNN equation `h_t = activation(W_ih * x_t + W_hh * h_{t-1} + b)`.\n",
        "# • `input_tensor` (`x_t`): The input vector at the current time step.\n",
        "# • `hidden_state` (`h_{t-1}`): The memory/context from the previous time step.\n",
        "# • `F.linear`: A functional way to apply a linear transformation (matrix multiplication + optional bias). Here, bias is added only in the hidden transformation for efficiency, effectively combining `b_ih` and `b_hh` into one `b`.\n",
        "# • `torch.tanh`: The non-linear activation function. It squashes the weighted sum into the range [-1, 1].\n",
        "\n",
        "# --- Potential Optimizations or Alternative Implementations ---\n",
        "# • `nn.Linear`: Using `nn.Linear` layers instead of manual `nn.Parameter` and `F.linear` can make the code slightly cleaner and potentially benefit from internal optimizations within PyTorch. Two `nn.Linear` layers would be needed, one for input and one for hidden state. (https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "# • Activation Function: ReLU or other activation functions could be used instead of tanh, although tanh is traditional for basic RNNs. ReLU might lead to faster training but could suffer from unbounded activations if not controlled. (https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
        "# • Initialization: Using more sophisticated weight initialization schemes (e.g., Xavier/Glorot or Kaiming/He initialization) instead of simple `randn` can lead to faster convergence and prevent vanishing/exploding gradients during the initial phases of training. (https://pytorch.org/docs/stable/nn.init.html)\n",
        "# • Bias Splitting: The bias could be split into `bias_ih` and `bias_hh`. While mathematically equivalent when combined, separating them might offer slightly more flexibility if specific regularization techniques were applied differently to input vs. recurrent connections. (https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
        "# • Matrix Multiplication Optimization: Matrix multiplications could be batched across time steps or combined using fused operations for better GPU utilization and throughput. (https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/)\n",
        "# • Sequence Handling: This implementation processes a single time step. For sequences, it must be iterated over manually, or wrapped in a custom RNN layer/class. Alternatively, PyTorch’s built-in `nn.RNN` module can be used. (https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
        "\n",
        "# --- Connection to Theoretical Concepts ---\n",
        "# • Recurrence Relation: This code directly implements the fundamental RNN recurrence relation: `h_t = f(W * [h_{t-1}, x_t] + b)`, where `f` is the activation function (tanh here). The hidden state `h_t` is computed based on the current input `x_t` and the previous hidden state `h_{t-1}`.\n",
        "# • Hidden State as Memory: The `hidden_state` tensor acts as the RNN's memory, carrying information from past time steps to influence the processing of future time steps. The quality of this memory is limited in simple RNNs (leading to vanishing gradients).\n",
        "# • Parameter Sharing: The same weight matrices (`weight_ih`, `weight_hh`) and bias (`bias`) are used across all time steps for processing the sequence. This makes the model parameter-efficient and allows it to generalize patterns across different sequence positions.\n",
        "# • Non-linearity: The `tanh` activation is crucial. Without it, the RNN would just be a series of linear transformations, collapsing into a single linear transformation, unable to model complex temporal dependencies.\n",
        "\n",
        "# Example Usage (Conceptual - requires iteration over a sequence)\n",
        "# batch_size = 5\n",
        "# input_size = 10\n",
        "# hidden_size = 20\n",
        "# seq_len = 15\n",
        "#\n",
        "# rnn_cell = SimpleRNNCell(input_size, hidden_size)\n",
        "# input_sequence = torch.randn(seq_len, batch_size, input_size) # Example input sequence\n",
        "# h_t = torch.zeros(batch_size, hidden_size) # Initial hidden state\n",
        "#\n",
        "# outputs = []\n",
        "# for t in range(seq_len):\n",
        "#     x_t = input_sequence[t] # Get input for time step t\n",
        "#     h_t = rnn_cell(x_t, h_t) # Compute next hidden state\n",
        "#     outputs.append(h_t)\n",
        "#\n",
        "# # 'outputs' now contains the hidden state for each time step.\n",
        "# # final_output = torch.stack(outputs, dim=0) # Shape: (seq_len, batch_size, hidden_size)\n",
        "\n",
        "print(\"Implemented Basic RNN Cell.\")\n",
        "\n",
        "# References\n",
        "\n",
        "# https://medium.com/data-science/recurrent-neural-networks-rnns-3f06d7653a85\n",
        "# https://d2l.ai/chapter_recurrent-neural-networks/index.html\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. LSTM Cell Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Combine weights for all four gates (input, forget, cell, output) into single weight matrices for efficiency. This allows for a single matrix\n",
        "        # multiplication for both input-to-hidden and hidden-to-hidden transformations, which is often optimized in deep learning libraries.\n",
        "        # The total size is 4 * hidden_size because each of the four gates needs a transformation resulting in a vector of size hidden_size.\n",
        "\n",
        "        # Weights for input-to-gate transformations (W_i, W_f, W_g, W_o combined)\n",
        "        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n",
        "        # Weights for hidden-to-gate transformations (U_i, U_f, U_g, U_o combined)\n",
        "        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n",
        "        # Biases for the gates (b_i, b_f, b_g, b_o combined)\n",
        "        # Use separate biases for the input-to-gate and hidden-to-gate transformations, although they are often\n",
        "        # combined into a single bias term per gate in practice (like PyTorch's default).\n",
        "        self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))\n",
        "        self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))\n",
        "\n",
        "    def forward(self, input_tensor, state):\n",
        "        # Unpack the previous hidden state and cell state\n",
        "        h_prev, c_prev = state\n",
        "\n",
        "        # Compute the linear transformations for all gates at once.\n",
        "        # Calculate W * x_t + b_ih\n",
        "        input_gates = F.linear(input_tensor, self.weight_ih, self.bias_ih)\n",
        "        # Calculate U * h_{t-1} + b_hh\n",
        "        hidden_gates = F.linear(h_prev, self.weight_hh, self.bias_hh)\n",
        "        # Combine the transformations\n",
        "        gates = input_gates + hidden_gates\n",
        "\n",
        "        # Split the combined gate tensor into four individual gate tensors.\n",
        "        # chunk(4, dim=1) splits the tensor along dimension 1 (the feature dimension) into 4 equal parts.\n",
        "        # i: input gate, f: forget gate, g: cell gate (candidate values), o: output gate\n",
        "        i, f, g, o = gates.chunk(4, dim=1)\n",
        "\n",
        "        # Apply appropriate activation functions to each gate.\n",
        "        # Sigmoid for gates (i, f, o) to scale values between 0 and 1, controlling flow.\n",
        "        # Tanh for the cell candidate gate (g) to scale values between -1 and 1.\n",
        "        i = torch.sigmoid(i) # Input gate: Controls how much new info (g) is added.\n",
        "        f = torch.sigmoid(f) # Forget gate: Controls how much old info (c_prev) is kept.\n",
        "        g = torch.tanh(g)    # Cell gate: Computes candidate values for the cell state.\n",
        "        o = torch.sigmoid(o) # Output gate: Controls how much of the cell state is exposed.\n",
        "\n",
        "        # Calculate the next cell state (c_t)\n",
        "        # c_t = f_t * c_{t-1} + i_t * g_t\n",
        "        # Element-wise multiplication (*) is used.\n",
        "        c_next = f * c_prev + i * g\n",
        "\n",
        "        # Calculate the next hidden state (h_t)\n",
        "        # h_t = o_t * tanh(c_t)\n",
        "        # Apply tanh to the new cell state before multiplying by the\n",
        "        # output gate. This keeps the hidden state values bounded.\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "# --- Annotations ---\n",
        "# • The `LSTMCell` class defines the computation for one time step, similar to the RNN cell, but with more complex internal logic.\n",
        "# • `__init__`: Initializes parameters. Crucially, the weight and bias parameters are 4 times the `hidden_size` because they contain parameters for all four gates (input `i`, forget `f`, cell candidate `g`, output `o`) concatenated together.\n",
        "# • `weight_ih`: Contains `W_ii, W_if, W_ig, W_io` concatenated.\n",
        "# • `weight_hh`: Contains `W_hi, W_hf, W_hg, W_ho` concatenated.\n",
        "# • `bias_ih`, `bias_hh`: Contain corresponding concatenated biases `b_ii, b_if, b_ig, b_io` and `b_hi, b_hf, b_hg, b_ho`.\n",
        "# • `forward`: Takes the current input `input_tensor` (`x_t`) and a tuple `state` containing the previous hidden state (`h_{t-1}`) and previous cell state (`c_{t-1}`).\n",
        "# • Gate Computations: First, linear transformations are applied to `x_t` and `h_{t-1}` using the combined weights/biases. The result `gates` contains the pre-activation values for all four gates.\n",
        "# • `gates.chunk(4, dim=1)`: Splits the result into four tensors, one for each gate (`i`, `f`, `g`, `o`).\n",
        "# • Activation Functions: Sigmoid (`torch.sigmoid`) is applied to `i`, `f`, `o` to make them act like gates (values between 0 and 1, controlling information flow). Tanh (`torch.tanh`) is applied to `g` to create candidate values for the cell state (values between -1 and 1).\n",
        "# • Cell State Update (`c_next`): This is the core of the LSTM's memory. `f * c_prev` scales the old cell state (deciding what to forget), and `i * g` scales the new candidate values (deciding what new information to add). `c_t = f_t ⊙ c_{t-1} + i_t ⊙ g_t`.\n",
        "# • Hidden State Update (`h_next`): The output gate `o` filters the (tanh-activated) new cell state `c_next` to produce the hidden state `h_t` that is passed to the next time step and potentially used as the output for the current step. `h_t = o_t ⊙ tanh(c_t)`.\n",
        "# • Return Value: Returns both the new hidden state (`h_next`) and the new cell state (`c_next`).\n",
        "\n",
        "# --- Potential Optimizations or Alternative Implementations ---\n",
        "# • `nn.Linear`: Similar to the RNN, using `nn.Linear(input_size + hidden_size, 4 * hidden_size)` on a concatenated input `[input_tensor, h_prev]` could replace the manual `F.linear` calls and potentially be optimized by PyTorch. (https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n",
        "# • Peephole Connections: An optional modification where the gate calculations also depend directly on the cell state (`c_{t-1}` or `c_t`). This adds extra parameters but can sometimes improve performance on specific tasks. E.g., `f = torch.sigmoid(..., W_fc @ c_prev)`. (https://ieeexplore.ieee.org/document/861302)\n",
        "# • Initialization: Careful initialization (e.g., setting forget gate bias initially high, around 1.0) is often recommended to encourage the LSTM to enhance its performance across various tasks.(https://proceedings.mlr.press/v37/jozefowicz15.pdf)\n",
        "# • Layer Normalization: Applying Layer Normalization to the gate activations or cell state can sometimes stabilize training and improve performance, especially in deeper LSTMs. (https://arxiv.org/abs/1607.06450)\n",
        "\n",
        "# --- Connection to Theoretical Concepts ---\n",
        "# • Gating Mechanisms: The core innovation of LSTMs. The input (`i`), forget (`f`), and output (`o`) gates are learnable functions that dynamically control the flow of information, addressing the vanishing/exploding gradient problem.\n",
        "# • Cell State (`c`): Acts as the main information highway, carrying information potentially over long durations. Its updates are additive (controlled by `i` and `f`), making it less prone to vanishing gradients compared to the multiplicative updates in simple RNNs.\n",
        "# • Forget Gate (`f`): Allows the LSTM to explicitly discard irrelevant information from the cell state (`f_t ⊙ c_{t-1}`).\n",
        "# • Input Gate (`i`): Allows the LSTM to explicitly control which new information (from the candidate `g_t`) is added to the cell state (`i_t ⊙ g_t`).\n",
        "# • Output Gate (`o`): Controls which parts of the cell state are filtered and exposed in the hidden state (`h_t`), which is used for predictions or passed to the next layer/time step.\n",
        "# • Mitigation of Vanishing Gradients: The additive nature of the cell state update and the gating mechanisms allow gradients to flow back through time more effectively than in simple RNNs.\n",
        "\n",
        "# Example Usage (Conceptual - requires iteration over a sequence)\n",
        "# batch_size = 5\n",
        "# input_size = 10\n",
        "# hidden_size = 20\n",
        "# seq_len = 15\n",
        "#\n",
        "# lstm_cell = LSTMCell(input_size, hidden_size)\n",
        "# input_sequence = torch.randn(seq_len, batch_size, input_size)\n",
        "# h_t = torch.zeros(batch_size, hidden_size) # Initial hidden state\n",
        "# c_t = torch.zeros(batch_size, hidden_size) # Initial cell state\n",
        "#\n",
        "# outputs = []\n",
        "# for t in range(seq_len):\n",
        "#     x_t = input_sequence[t]\n",
        "#     h_t, c_t = lstm_cell(x_t, (h_t, c_t)) # Note: state is now a tuple (h, c)\n",
        "#     outputs.append(h_t)\n",
        "#\n",
        "# # final_output = torch.stack(outputs, dim=0)\n",
        "\n",
        "print(\"Implemented LSTM Cell.\")\n",
        "\n",
        "# References\n",
        "\n",
        "# https://d2l.ai/chapter_recurrent-modern/lstm.html\n",
        "# https://medium.com/@samina.amin/understanding-lstms-lstm-implementation-from-scratch-18965a150eca\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Self-Attention Mechanism Implementation (Scaled Dot-Product Attention)\n",
        "# ==============================================================================\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim): # Usually d_k = d_v = hidden_dim / num_heads\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        # Store the scaling factor directly. The square root of the\n",
        "        # key dimension (d_k) is used to prevent the dot products from growing too large,\n",
        "        # which could push the softmax function into regions with very small gradients.\n",
        "        self.scale_factor = math.sqrt(hidden_dim)\n",
        "        # Use dropout for regularization within the attention mechanism.\n",
        "        # Applying dropout to the attention weights before multiplying with V helps prevent overfitting to specific attention patterns.\n",
        "        # ( Dropout probability could be a parameter, default is often 0.1 )\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # Softmax is applied along the last dimension (key dimension) to get attention weights that sum to 1 for each query.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # 1. Compute dot products between Query and Key: Q @ K.T\n",
        "        # Transpose the last two dimensions of the key tensor.\n",
        "        # Result shape: (batch_size, ..., seq_len_q, seq_len_k)\n",
        "        # Use torch.matmul for batch matrix multiplication.\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "        # 2. Scale the scores\n",
        "        # Divide by the square root of the key dimension (d_k).\n",
        "        scaled_attention_scores = attention_scores / self.scale_factor\n",
        "\n",
        "        # 3. Apply mask\n",
        "        if mask is not None:\n",
        "            # Fill masked positions with a very small number (-inf).\n",
        "            # This ensures that after softmax, these positions will have near-zero probability.\n",
        "            # The mask tensor provides 0s where it is wanted to mask, so it is directly used.\n",
        "            # with fill_ where the condition (mask == 0) is true.\n",
        "            # Make sure mask has compatible dimensions (broadcastable).\n",
        "            # Example mask shapes: (batch, 1, 1, seq_len_k) for padding mask or\n",
        "            # (batch, 1, seq_len_q, seq_len_k) for combined/look-ahead mask.\n",
        "            scaled_attention_scores = scaled_attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        # 4. Apply softmax to get attention weights\n",
        "        # Softmax is applied on the last dimension (seq_len_k), so weights sum to 1 for each query.\n",
        "        # Shape: (batch_size, ..., seq_len_q, seq_len_k)\n",
        "        attention_weights = self.softmax(scaled_attention_scores)\n",
        "\n",
        "        # Apply dropout to attention weights for regularization.\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # 5. Compute the weighted sum of Values\n",
        "        # Multiply attention weights by the Value tensor.\n",
        "        # Result shape: (batch_size, ..., seq_len_q, value_dim)\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "# --- Annotations ---\n",
        "# • The `ScaledDotProductAttention` class implements the core attention calculation.\n",
        "# • `__init__`: Stores the scaling factor (`sqrt(d_k)`) and initializes dropout and softmax layers.\n",
        "# • `scale_factor`: Crucial for stable training, prevents dot products from becoming too large, especially with high `hidden_dim`.\n",
        "# • `forward`: Takes `query`, `key`, `value` tensors, and an optional `mask`.\n",
        "# • `torch.matmul(query, key.transpose(-2, -1))`: Computes the raw attention scores by measuring the similarity (dot product) between each query vector and all key vectors.\n",
        "# • Scaling: Divides the raw scores by `scale_factor`.\n",
        "# • Masking: If a `mask` is provided (e.g., for padding or future tokens in decoders), it sets the scores at masked positions to a very large negative number (`float('-inf')`). This makes their probability effectively zero after softmax.\n",
        "# • `self.softmax(dim=-1)`: Converts the scaled (and potentially masked) scores into probability distributions (attention weights) across the key/value sequence for each query. The weights sum to 1 along the key sequence length dimension (`seq_len_k`).\n",
        "# • Dropout: Applied to the `attention_weights` to prevent the model from relying too heavily on a few specific input positions.\n",
        "# • `torch.matmul(attention_weights, value)`: Computes the final output by taking a weighted sum of the `value` vectors, where the weights are the computed `attention_weights`. Each output vector is a blend of value vectors, weighted by their relevance to the corresponding query.\n",
        "# • Return Value: Returns the attention output (context vector) and the attention weights (useful for analysis/visualization).\n",
        "\n",
        "# --- Potential Optimizations or Alternative Implementations ---\n",
        "# • Fused Kernels: Libraries like xformers or PyTorch 2.0+ provide fused attention implementations (e.g., `torch.nn.functional.scaled_dot_product_attention`) that combine scaling, masking, softmax, and dropout into a single, highly optimized GPU kernel. This is significantly faster and uses less memory than the manual steps shown here. (https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n",
        "# • Different Attention Types: This is scaled dot-product attention. Alternatives exist:\n",
        "#     - Additive Attention (Bahdanau): Uses a feed-forward network to compute alignment scores. Can sometimes work better for lower dimensions but is often slower. `score(q, k) = v_a^T * tanh(W_q*q + W_k*k)` (https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html)\n",
        "#     - Cosine Similarity: Use cosine similarity instead of dot product. (https://arxiv.org/pdf/1702.05870)\n",
        "# • Multi-Head Attention: This implementation is single-head. Multi-head attention (covered later) projects Q, K, V into multiple subspaces (\"heads\"), computes attention independently in each head, and concatenates the results. This allows the model to attend to information from different representation subspaces simultaneously. (https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html)\n",
        "\n",
        "# --- Connection to Theoretical Concepts ---\n",
        "# • Attention Mechanism: Directly implements the concept of attention, allowing the model to focus on relevant parts of the input sequence (represented by values) based on a query and keys.\n",
        "# • Query, Key, Value Abstraction: Represents information processing as a database retrieval analogy. The `query` asks a question, `keys` provide searchable identifiers for information, and `values` contain the actual information content. Attention finds which keys best match the query and retrieves a weighted combination of the corresponding values.\n",
        "# • Softmax for Weighting: Softmax converts similarity scores into a probability distribution, providing normalized weights for combining the value vectors.\n",
        "# • Scaling Factor: Addresses the issue that dot products grow in magnitude with dimensionality. Scaling ensures the variance of the dot products remains constant regardless of `d_k`, keeping softmax inputs in a reasonable range.\n",
        "# • Self-Attention: When Q, K, and V are derived from the same input sequence (often through linear projections), it's called self-attention. This allows different positions in the sequence to interact and exchange information based on content similarity.\n",
        "\n",
        "# Example Usage (Conceptual)\n",
        "# batch_size = 4\n",
        "# seq_len = 10\n",
        "# hidden_dim = 64 # d_k, d_v usually this size\n",
        "# embed_dim = 512 # Dimension of input embeddings\n",
        "#\n",
        "# # In a real scenario, Q, K, V are often linear projections of the input\n",
        "# input_embeddings = torch.randn(batch_size, seq_len, embed_dim)\n",
        "# W_q = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
        "# W_k = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
        "# W_v = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
        "#\n",
        "# Q = W_q(input_embeddings) # Shape: (batch, seq_len, hidden_dim)\n",
        "# K = W_k(input_embeddings) # Shape: (batch, seq_len, hidden_dim)\n",
        "# V = W_v(input_embeddings) # Shape: (batch, seq_len, hidden_dim)\n",
        "#\n",
        "# attention_layer = ScaledDotProductAttention(hidden_dim=hidden_dim)\n",
        "# # Optional mask (e.g., ignore padding tokens at index > 7)\n",
        "# mask = torch.ones(batch_size, 1, seq_len, seq_len) # (Batch, Head, Seq_Q, Seq_K)\n",
        "# mask[:, :, :, 8:] = 0 # Mask out last 2 positions\n",
        "#\n",
        "# attention_output, attention_weights = attention_layer(Q, K, V, mask=mask)\n",
        "# print(attention_output.shape) # Should be (batch_size, seq_len, hidden_dim)\n",
        "# print(attention_weights.shape) # Should be (batch_size, seq_len, seq_len)\n",
        "\n",
        "print(\"Implemented Scaled Dot-Product Attention (Self-Attention basis).\")\n",
        "\n",
        "# References\n",
        "\n",
        "# https://classic.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html\n",
        "# https://classic.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html#equation-eq-softmax-qk-v\n",
        "# https://medium.com/@vmirly/tutorial-on-scaled-dot-product-attention-with-pytorch-implementation-from-scratch-66ed898bf817\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Sequence-to-Sequence (Seq2Seq) Model with Attention\n",
        "# ==============================================================================\n",
        "\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer to convert input token indices into dense vectors.\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_dim)\n",
        "        # Use PyTorch's nn.LSTM layer. It processes the entire sequence.\n",
        "        # `batch_first=True` makes input/output tensors shape (batch, seq, feature).\n",
        "        # Dropout is applied between LSTM layers if num_layers > 1.\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0) # NOTE: Dropout is ignored if num_layers=1, so it is disabled in that case.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        # 1. Embed the input sequence\n",
        "        # Shape: (batch_size, seq_len) -> (batch_size, seq_len, embed_dim)\n",
        "        embedded = self.dropout(self.embedding(input_seq))\n",
        "\n",
        "        # 2. Pass embedded sequence through LSTM\n",
        "        # hidden_state tuple contains (final hidden state, final cell state)\n",
        "        # Shape of h_n, c_n: (num_layers * num_directions, batch_size, hidden_size)\n",
        "        # Shape of outputs: (batch_size, seq_len, num_directions * hidden_size)\n",
        "        encoder_outputs, hidden_state = self.lstm(embedded) # Not provide initial h/c, defaults to zeros.\n",
        "\n",
        "        # The encoder_outputs contain the hidden state for every time step.\n",
        "        # The hidden_state tuple contains the final hidden and cell states.\n",
        "        return encoder_outputs, hidden_state\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        # No learnable parameters needed for simple dot-product attention.\n",
        "        # Could use ScaledDotProductAttention or Bahdanau (additive) attention here too.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        # Store hidden_dim if scaling or other methods are used later.\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.scale = 1 / math.sqrt(hidden_dim)  # For scaled dot-product attention\n",
        "\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        # Calculate alignment scores (dot product)\n",
        "        # decoder_hidden shape: (batch, hidden)\n",
        "        # encoder_outputs shape: (batch, seq_len_enc, hidden)\n",
        "        # Want scores shape: (batch, seq_len_enc)\n",
        "        # Need to match dimensions for batch matmul:\n",
        "        # Query: (batch, 1, hidden)\n",
        "        # Keys:  (batch, seq_len_enc, hidden) -> transpose -> (batch, hidden, seq_len_enc)\n",
        "        # Result: (batch, 1, seq_len_enc) -> squeeze -> (batch, seq_len_enc)\n",
        "\n",
        "        # Add a dimension to decoder_hidden for matmul\n",
        "        query = decoder_hidden.unsqueeze(1) # Shape: (batch_size, 1, hidden_dim)\n",
        "\n",
        "        # Calculate scores\n",
        "        scores = torch.bmm(query, encoder_outputs.transpose(1, 2)).squeeze(1) * self.scale # Shape: (batch_size, 1, seq_len_enc)\n",
        "        scores = scores.squeeze(1) # Shape: (batch_size, seq_len_enc)\n",
        "\n",
        "        # Apply softmax to get weights\n",
        "        attention_weights = self.softmax(scores) # Shape: (batch_size, seq_len_enc)\n",
        "\n",
        "        # Calculate context vector (weighted sum of encoder outputs)\n",
        "        # weights shape: (batch, 1, seq_len_enc) for bmm\n",
        "        # values shape: (batch, seq_len_enc, hidden)\n",
        "        # result shape: (batch, 1, hidden) -> squeeze -> (batch, hidden)\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs) # Shape: (batch_size, 1, hidden_dim)\n",
        "        context_vector = context_vector.squeeze(1) # Shape: (batch_size, hidden_dim)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class DecoderLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super(DecoderLSTMWithAttention, self).__init__()\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_dim)\n",
        "        # Attention layer instance.\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        # LSTM Cell to process one step at a time.\n",
        "        # The input to the LSTM cell will be the concatenation of the embedded\n",
        "        # previous output token and the context vector from attention.\n",
        "        self.lstm_cell = nn.LSTMCell(embed_dim + hidden_dim, hidden_dim)\n",
        "        # Output linear layer to map the combined output to vocab scores.\n",
        "        # Input size is hidden_dim (from LSTM) + hidden_dim (from context) + embed_dim\n",
        "        # Combine hidden state and context vector\n",
        "        self.fc_out = nn.Linear(hidden_dim + hidden_dim, output_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, decoder_input, hidden_state, encoder_outputs):\n",
        "        # 1. Embed the input token\n",
        "        # Shape: (batch_size) -> (batch_size, embed_dim)\n",
        "        embedded = self.dropout(self.embedding(decoder_input))\n",
        "\n",
        "        # 2. Calculate attention weights and context vector\n",
        "        # Use the previous hidden state (h_prev) as the query.\n",
        "        # hidden_state[0] is h, hidden_state[1] is c.\n",
        "        h_prev = hidden_state[0]\n",
        "        context_vector, attention_weights = self.attention(h_prev, encoder_outputs)\n",
        "        # context_vector shape: (batch_size, hidden_dim)\n",
        "        # attention_weights shape: (batch_size, seq_len_enc)\n",
        "\n",
        "        # 3. Combine embedded input and context vector\n",
        "        # This combined vector will be the input to the LSTM cell.\n",
        "        # Shape: (batch_size, embed_dim + hidden_dim)\n",
        "        lstm_input = torch.cat((embedded, context_vector), dim=1)\n",
        "\n",
        "        # 4. Pass through LSTM Cell\n",
        "        # hidden_state is the tuple (h_prev, c_prev)\n",
        "        hidden_state = self.lstm_cell(lstm_input, hidden_state)\n",
        "        # hidden_state is now the tuple (h_next, c_next)\n",
        "\n",
        "        # 5. Compute output prediction\n",
        "        # Combine the new hidden state and the context vector before passing to the final fully connected layer.\n",
        "        # This gives the output layer access to both the current decoder state and the relevant encoder context.\n",
        "        h_next = hidden_state[0]\n",
        "        output_combined = torch.cat((h_next, context_vector), dim=1)\n",
        "        # Shape: (batch_size, hidden_dim + hidden_dim)\n",
        "\n",
        "        prediction = self.fc_out(output_combined)\n",
        "        # Shape: (batch_size, output_vocab_size)\n",
        "\n",
        "        return prediction, hidden_state, attention_weights\n",
        "\n",
        "# --- Overall Seq2Seq Model ---\n",
        "\n",
        "class Seq2SeqAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2SeqAttention, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device # Necessary for creating initial input tensor\n",
        "\n",
        "        # Ensure hidden dimensions match\n",
        "        assert encoder.hidden_dim == decoder.hidden_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        # Assume num_layers=1 for simplicity in passing state\n",
        "        assert encoder.num_layers == 1, \"Encoder num_layers > 1 not handled simply in this basic decoder setup.\"\n",
        "        # If num_layers > 1, need to handle selecting the correct layer's state\n",
        "        # If using multi-layer LSTM, need to pass only final layer's hidden state or merge states.\n",
        "\n",
        "    def forward(self, input_seq, target_seq, teacher_forcing_ratio=0.5):\n",
        "        batch_size = target_seq.shape[0]\n",
        "        trg_len = target_seq.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_vocab_size\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # 1. Pass input sequence through encoder\n",
        "        encoder_outputs, hidden_state = self.encoder(input_seq)\n",
        "        # hidden_state is (h, c), each shape (num_layers=1, batch, hidden)\n",
        "        # Need (batch, hidden) for the decoder cell, so squeeze num_layers dim\n",
        "        decoder_hidden = (hidden_state[0].squeeze(0), hidden_state[1].squeeze(0))\n",
        "\n",
        "        # 2. Prepare initial decoder input (<sos> token)\n",
        "        # Typically, the first input to the decoder is the start-of-sequence token.\n",
        "        # Assuming <sos> token index is 0 for simplicity.\n",
        "        decoder_input = target_seq[:, 0] # Use first token of target for <sos> like prompt\n",
        "        # Or create explicitly: torch.zeros(batch_size, dtype=torch.long).to(self.device)\n",
        "        # Assume <sos> token is at position 0 in the target sequence. Adjust if using fixed token ID.\n",
        "\n",
        "        # 3. Decoding loop\n",
        "        for t in range(trg_len):\n",
        "            # Run one step of the decoder\n",
        "            prediction, decoder_hidden, _ = self.decoder(decoder_input,\n",
        "                                                         decoder_hidden,\n",
        "                                                         encoder_outputs)\n",
        "\n",
        "            # Store the prediction\n",
        "            outputs[:, t, :] = prediction # Shape (batch, vocab_size)\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "\n",
        "            # Get the highest predicted token\n",
        "            top1 = prediction.argmax(1) # Shape: (batch_size)\n",
        "\n",
        "            # If teacher forcing, use actual next target token; else use prediction\n",
        "            # decoder_input = target_seq[:, t] if teacher_force and t < trg_len -1 else top1 # Commented out redundant line\n",
        "\n",
        "            # Ensure input is not out of bounds if using target_seq\n",
        "            if t < trg_len -1: # Check index before accessing next target\n",
        "                decoder_input = target_seq[:, t+1] if teacher_force else top1\n",
        "                # Use teacher forcing or model's own prediction. Avoid index out of bounds by checking t < trg_len - 1.\n",
        "                # Use teacher forcing if available, otherwise use model prediction. Prevent out-of-bounds access at last time step.\n",
        "            else: # Last step, doesn't matter much what input is for next iteration\n",
        "                decoder_input = top1\n",
        "\n",
        "        return outputs # The outputs are raw logits. Use nn.CrossEntropyLoss (which applies softmax internally).\n",
        "\n",
        "# --- Annotations ---\n",
        "# • Architecture: Consists of an `EncoderLSTM`, a `DecoderLSTMWithAttention`, and an `Attention` module.\n",
        "# • `EncoderLSTM`: Takes input sequence indices, embeds them, and processes them with an `nn.LSTM`. It returns all hidden states (`encoder_outputs`) and the final hidden and cell states.\n",
        "# • `Attention`: Computes attention weights. Here, a simple dot-product attention is used. It takes the current decoder hidden state (`decoder_hidden`) as the query and the `encoder_outputs` as keys and values. It returns a `context_vector` (weighted sum of encoder outputs) and `attention_weights`.\n",
        "# • `DecoderLSTMWithAttention`: Generates the output sequence one token at a time.\n",
        "#     - Takes the previously generated token (`decoder_input`), the previous decoder hidden/cell state (`hidden_state`), and all `encoder_outputs`.\n",
        "#     - Embeds the `decoder_input`.\n",
        "#     - Uses the `attention` module with the previous hidden state `h_prev` to compute the `context_vector`.\n",
        "#     - Concatenates the `embedded` input and the `context_vector`. This combined vector is fed into the `nn.LSTMCell` along with the previous `hidden_state`.\n",
        "#     - The `nn.LSTMCell` produces the new hidden/cell state (`h_next`, `c_next`).\n",
        "#     - Concatenates the `h_next` and the `context_vector` to form the input for the final linear layer (`fc_out`).\n",
        "#     - `fc_out` projects this combined vector to the size of the output vocabulary, producing prediction scores.\n",
        "# • `Seq2SeqAttention`: The main class orchestrating the process.\n",
        "#     - Initializes encoder and decoder.\n",
        "#     - `forward`: Takes source and target sequences.\n",
        "#     - Encodes the `input_seq` to get `encoder_outputs` and the initial `decoder_hidden` state (the encoder's final state).\n",
        "#     - Initializes the first `decoder_input` (e.g., <sos> token).\n",
        "#     - Iterates `trg_len` times:\n",
        "#         - Calls the `decoder`'s forward method for one step.\n",
        "#         - Stores the output prediction.\n",
        "#         - Implements Teacher Forcing: With probability `teacher_forcing_ratio`, the next `decoder_input` is the *actual* next token from `target_seq`. Otherwise, it's the token with the highest score from the current `prediction` (greedy decoding).\n",
        "#     - Returns all predictions for the sequence.\n",
        "\n",
        "# --- Connection to Theoretical Concepts ---\n",
        "# • Sequence-to-Sequence Learning: The Seq2Seq architecture models a conditional sequence generation problem:\n",
        "#     Given an input sequence X = (x₁, x₂, ..., xₙ), the model learns to generate an output sequence Y = (y₁, y₂, ..., yₘ).\n",
        "#     This is particularly useful in tasks like machine translation, summarization, and question answering.\n",
        "#\n",
        "# • Encoder-Decoder Framework:\n",
        "#     - The encoder compresses the input sequence into a fixed-dimensional context vector (final hidden state).\n",
        "#     - The decoder unfolds the output sequence conditioned on this context.\n",
        "#     - However, relying solely on the final encoder state limits performance on long sequences (information bottleneck).\n",
        "#\n",
        "# • Attention Mechanism:\n",
        "#     - Introduced by Bahdanau et al. (2015), attention allows the decoder to dynamically focus on different parts of the input sequence during each decoding step.\n",
        "#     - https://arxiv.org/abs/1409.0473\n",
        "#     - This removes the fixed-length bottleneck and improves gradient flow during training.\n",
        "#     - The attention module acts as a soft alignment function, learning where to \"look\" in the input when generating each token.\n",
        "#\n",
        "# • Dot-Product Attention:\n",
        "#     - A simple and efficient way to compute similarity between the decoder's current hidden state (query) and each encoder output (keys).\n",
        "#     - The similarity scores are normalized using softmax and used to compute a weighted sum (context vector) of encoder outputs (values).\n",
        "#\n",
        "# • Context Vector:\n",
        "#     - Represents a weighted summary of the input sequence tailored to the current decoding step.\n",
        "#     - It acts as dynamic memory that gives the decoder access to the full input sequence rather than a single summary.\n",
        "#\n",
        "# • Teacher Forcing:\n",
        "#     - A technique used during training where the ground truth token is fed as the next input to the decoder, rather than the model’s own prediction.\n",
        "#     - Helps speed up convergence and stabilizes training, especially in early stages.\n",
        "#     - A stochastic version is used here, controlled by `teacher_forcing_ratio`.\n",
        "#\n",
        "# • Autoregressive Decoding:\n",
        "#     - The decoder generates one token at a time, and each prediction conditions on previous tokens.\n",
        "#     - This requires looping over the target sequence length during training/inference.\n",
        "#\n",
        "# • Attention Weights (Alignment Scores):\n",
        "#     - These can be visualized to interpret which parts of the source sentence the model is attending to for each generated word.\n",
        "#     - Useful for explainability and debugging.\n",
        "#\n",
        "# • Model Modularity:\n",
        "#     - The model separates encoder, decoder, and attention, allowing flexible experimentation (e.g., replacing dot-product with additive attention, or encoder LSTM with Transformer).\n",
        "#\n",
        "# • RNN Limitations & Transition to Transformers:\n",
        "#     - Seq2Seq models with attention address many limitations of vanilla RNNs but still suffer from slow inference and difficulty modeling long-range dependencies.\n",
        "#     - These limitations led to the development of the Transformer architecture, which replaces recurrence with full attention mechanisms.\n",
        "\n",
        "\n",
        "# --- Potential Optimizations or Alternative Implementations ---\n",
        "# • Attention Mechanism: Replace simple dot-product attention with Scaled Dot-Product or Additive (Bahdanau) attention (from Part 3, previous part). This often involves adding linear layers to project decoder hidden state and encoder outputs before calculating scores.\n",
        "# • Bidirectional Encoder: Use a bidirectional LSTM in the encoder (`bidirectional=True`). The `encoder_outputs` will have dimension `2 * hidden_dim`. The decoder's initial state needs to be adapted (e.g., by combining forward and backward final states, perhaps with a linear layer). (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "# • Multi-Layer RNNs: Use `num_layers > 1` in both encoder and decoder for deeper processing. Handling the hidden states between encoder and decoder requires care (usually only the final layer's state initializes the decoder, or states are combined).\n",
        "# • Decoder Cell: Custom `LSTMCell` could be used instead of `nn.LSTMCell`.\n",
        "# • Beam Search Decoding: For inference (not training), replace greedy decoding (taking `argmax`) with beam search to find potentially better output sequences by keeping track of multiple candidate sequences. (https://www.width.ai/post/what-is-beam-search#:~:text=Beam%20Search:%20Using%20Conditional%20Probability&text=The%20beam%20search%20algorithm%20selects,to%20use%20beam%20search%20effectively.)\n",
        "# • Input Feeding: A common technique where the previous attention context vector is also fed as input to the next time step's decoder LSTM cell, in addition to the embedded previous token and the current context vector. `lstm_input = torch.cat((embedded, context_vector, prev_context_vector), dim=1)`. (https://github.com/facebookresearch/fairseq/blob/main/fairseq/models/lstm.py)\n",
        "# • Packed Sequences: For handling variable-length sequences efficiently without explicit masking in the RNNs. (https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html)(https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)\n",
        "print(\"Implemented Seq2Seq Model with Attention.\")\n",
        "\n",
        "# References\n",
        "\n",
        "# https://classic.d2l.ai/chapter_recurrent-modern/seq2seq.html\n",
        "# https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html\n",
        "# https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Code Fragments for Modifications\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 5.1 Add Bidirectionality to an RNN/LSTM ---\n",
        "print(\"\\n--- 5.1 Bidirectionality Example ---\")\n",
        "# (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "# To make an RNN or LSTM bidirectional using nn.Module layers:\n",
        "# Set `bidirectional=True` during initialization.\n",
        "\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "\n",
        "# Example with nn.LSTM\n",
        "bidirectional_lstm = nn.LSTM(\n",
        "    input_size=embed_dim,\n",
        "    hidden_size=hidden_dim,\n",
        "    num_layers=num_layers,\n",
        "    batch_first=True,      # Input shape: (batch, seq, feature)\n",
        "    bidirectional=True     # Enable bidirectionality\n",
        ")\n",
        "\n",
        "# Example input\n",
        "batch_size = 4\n",
        "seq_len = 10\n",
        "input_tensor = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Forward pass\n",
        "outputs, (h_n, c_n) = bidirectional_lstm(input_tensor)\n",
        "\n",
        "# Explanation of Output Shapes:\n",
        "# - `outputs`: Contains the concatenated hidden states from both forward and backward directions for each time step from the last layer.\n",
        "#   Shape: (batch_size, seq_len, hidden_dim * 2)\n",
        "# - `h_n`: Final hidden states for each layer and each direction.\n",
        "#   Shape: (num_layers * 2, batch_size, hidden_dim)\n",
        "#   The first `num_layers` entries are forward, the next `num_layers` are backward.\n",
        "# - `c_n`: Final cell states for each layer and each direction.\n",
        "#   Shape: (num_layers * 2, batch_size, hidden_dim)\n",
        "\n",
        "print(f\"Bidirectional LSTM Output shape: {outputs.shape}\")\n",
        "print(f\"Bidirectional LSTM Final Hidden shape: {h_n.shape}\")\n",
        "print(f\"Bidirectional LSTM Final Cell shape: {c_n.shape}\")\n",
        "\n",
        "# If implementing manually with cells, need two separate cells/loops,\n",
        "# one processing the sequence forward and one backward, then concatenate results.\n",
        "\n",
        "\n",
        "# --- 5.2 Implement a GRU Cell instead of an LSTM ---\n",
        "print(\"\\n--- 5.2 GRU Cell Implementation ---\")\n",
        "# (https://d2l.ai/chapter_recurrent-modern/gru.html)\n",
        "# (https://medium.com/biased-algorithms/gated-recurrent-unit-explained-cd2c0a4d29f8)\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # GRU has two main gates: Reset (r) and Update (z).\n",
        "        # It also computes a candidate hidden state (n or h_tilde).\n",
        "        # Need weights for input-to-gate/candidate and hidden-to-gate/candidate.\n",
        "        # Combine weights for [reset, update] gates\n",
        "        self.weight_ih_gates = nn.Parameter(torch.randn(2 * hidden_size, input_size))\n",
        "        self.weight_hh_gates = nn.Parameter(torch.randn(2 * hidden_size, hidden_size))\n",
        "        self.bias_gates = nn.Parameter(torch.randn(2 * hidden_size))\n",
        "\n",
        "        # Combine weights for candidate hidden state (n)\n",
        "        self.weight_ih_cand = nn.Parameter(torch.randn(hidden_size, input_size))\n",
        "        self.weight_hh_cand = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.bias_cand = nn.Parameter(torch.randn(hidden_size))\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state):\n",
        "        # Linear transformations for gates (reset r, update z)\n",
        "        gates_ih = F.linear(input_tensor, self.weight_ih_gates)\n",
        "        gates_hh = F.linear(hidden_state, self.weight_hh_gates, self.bias_gates)\n",
        "        gate_values = gates_ih + gates_hh\n",
        "\n",
        "        # Split and apply sigmoid activation\n",
        "        r, z = torch.sigmoid(gate_values).chunk(2, dim=1)\n",
        "        # r: reset gate, z: update gate\n",
        "\n",
        "        # Linear transformations for candidate hidden state (n)\n",
        "        cand_ih = F.linear(input_tensor, self.weight_ih_cand)\n",
        "        # Apply reset gate (r) to the hidden part of the candidate calculation\n",
        "        cand_hh = F.linear(r * hidden_state, self.weight_hh_cand, self.bias_cand)\n",
        "        # Compute candidate hidden state with tanh activation\n",
        "        n = torch.tanh(cand_ih + cand_hh)\n",
        "\n",
        "        # Compute next hidden state\n",
        "        # h_t = (1 - z_t) * n_t + z_t * h_{t-1}\n",
        "        h_next = (1 - z) * n + z * hidden_state\n",
        "\n",
        "        return h_next\n",
        "\n",
        "# Conceptual Usage:\n",
        "# gru_cell = GRUCell(input_size=10, hidden_size=20)\n",
        "# h_prev = torch.zeros(5, 20) # batch_size=5\n",
        "# input_t = torch.randn(5, 10)\n",
        "# h_next = gru_cell(input_t, h_prev)\n",
        "# print(f\"GRU Cell Output shape: {h_next.shape}\")\n",
        "print(\"Implemented GRU Cell.\")\n",
        "\n",
        "# --- 5.3 Apply Masking for Variable-Length Sequences ---\n",
        "print(\"\\n--- 5.3 Masking Examples ---\")\n",
        "# (https://huggingface.co/transformers/v2.1.1/_modules/transformers/modeling_ctrl.html)\n",
        "\n",
        "# Masking is crucial when batching sequences of different lengths.\n",
        "# Pad sequences to the same length and use masks to ignore padded tokens.\n",
        "\n",
        "# Example: Padding Mask for Attention\n",
        "batch_size = 2\n",
        "max_seq_len = 5\n",
        "hidden_dim = 8\n",
        "# Example sequence lengths in a batch\n",
        "seq_lengths = torch.tensor([3, 5]) # Sequence 1 has length 3, Sequence 2 has length 5\n",
        "\n",
        "# Create a mask: 1 for real tokens, 0 for padding\n",
        "# (batch_size, max_seq_len)\n",
        "mask_simple = torch.arange(max_seq_len)[None, :] < seq_lengths[:, None]\n",
        "# -> tensor([[ True,  True,  True, False, False],\n",
        "#            [ True,  True,  True,  True,  True]])\n",
        "\n",
        "# For attention (like ScaledDotProductAttention), the mask often needs to be\n",
        "# broadcastable to the attention scores shape (batch, heads, seq_len_q, seq_len_k).\n",
        "# Assuming seq_len_q = seq_len_k = max_seq_len, heads = 1\n",
        "# Unsqueeze dimensions for heads (dim 1) and query sequence length (dim 2)\n",
        "attention_mask = mask_simple.unsqueeze(1).unsqueeze(2).float()\n",
        "# Shape: (batch_size, 1, 1, max_seq_len)\n",
        "# -> tensor([[[[1., 1., 1., 0., 0.]]],\n",
        "#            [[[1., 1., 1., 1., 1.]]]])\n",
        "\n",
        "print(\"Created Attention Mask:\", attention_mask)\n",
        "print(\"Attention Mask shape:\", attention_mask.shape)\n",
        "\n",
        "# Usage with ScaledDotProductAttention:\n",
        "# dummy_qkv = torch.randn(batch_size, 1, max_seq_len, hidden_dim)\n",
        "# attention_layer = ScaledDotProductAttention(hidden_dim)\n",
        "# output, weights = attention_layer(dummy_qkv, dummy_qkv, dummy_qkv, mask=attention_mask)\n",
        "# The `masked_fill(mask == 0, -inf)` inside the attention layer handles this.\n",
        "\n",
        "# Example: Masking in Loss Calculation\n",
        "# Assume model outputs predictions (batch, seq_len, vocab_size)\n",
        "# and targets are (batch, seq_len)\n",
        "\n",
        "# Dummy data\n",
        "outputs = torch.randn(batch_size, max_seq_len, 100) # vocab_size = 100\n",
        "targets = torch.randint(0, 100, (batch_size, max_seq_len))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction='none') # Compute loss per element\n",
        "\n",
        "# Calculate loss for all positions (including padding)\n",
        "# Need to reshape for CrossEntropyLoss: (N, C, ...) -> (N*seq_len, C)\n",
        "# Targets: (N, ...) -> (N*seq_len)\n",
        "loss = criterion(outputs.view(-1, 100), targets.view(-1))\n",
        "loss = loss.view(batch_size, max_seq_len) # Reshape back to (batch, seq_len)\n",
        "\n",
        "print(f\"Loss before masking (shape {loss.shape}):\\n{loss}\")\n",
        "\n",
        "# Apply the simple mask (True/False or 1/0) to zero out loss for padded positions\n",
        "# mask_simple shape: (batch_size, max_seq_len)\n",
        "masked_loss = loss * mask_simple.float()\n",
        "\n",
        "print(f\"Loss after masking:\\n{masked_loss}\")\n",
        "\n",
        "# Calculate the mean loss only over non-padded elements\n",
        "# Sum the masked loss and divide by the number of actual tokens\n",
        "mean_loss = masked_loss.sum() / mask_simple.float().sum()\n",
        "print(f\"Mean masked loss: {mean_loss.item()}\")\n",
        "\n",
        "\n",
        "# --- 5.4 Implement Multi-Head Attention ---\n",
        "print(\"\\n--- 5.4 Multi-Head Attention Implementation ---\")\n",
        "# (https://sanjayasubedi.com.np/deeplearning/multihead-attention-from-scratch/)\n",
        "# (https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Use single large linear layers for Q, K, V projections\n",
        "        # and split the result later. This can be more efficient than multiple smaller linear layers.\n",
        "\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim) # Projects input to Q (all heads)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim) # Projects input to K (all heads)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim) # Projects input to V (all heads)\n",
        "\n",
        "        # Scaled dot-product attention layer (single head calculation)\n",
        "        # The hidden_dim for the attention calculation is the head_dim\n",
        "        self.attention = ScaledDotProductAttention(hidden_dim=self.head_dim)\n",
        "\n",
        "        # Final linear layer after concatenating heads\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # LayerNorm is often used in Transformer blocks alongside MHA\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def split_heads(self, tensor, batch_size):\n",
        "        \"\"\" Reshapes tensor to separate heads. \"\"\"\n",
        "        # Input shape: (batch_size, seq_len, embed_dim)\n",
        "        # Output shape: (batch_size, num_heads, seq_len, head_dim)\n",
        "        return tensor.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, tensor, batch_size):\n",
        "        \"\"\" Combines heads back into embed_dim. \"\"\"\n",
        "        # Input shape: (batch_size, num_heads, seq_len, head_dim)\n",
        "        # Output shape: (batch_size, seq_len, embed_dim)\n",
        "        return tensor.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
        "\n",
        "    def forward(self, query_input, key_input, value_input, mask=None):\n",
        "        batch_size = query_input.shape[0]\n",
        "\n",
        "        # 1. Project input tensors to Q, K, V (for all heads)\n",
        "        Q = self.W_q(query_input)\n",
        "        K = self.W_k(key_input)\n",
        "        V = self.W_v(value_input)\n",
        "        # Shapes: (batch_size, seq_len_*, embed_dim)\n",
        "\n",
        "        # 2. Split embed_dim into num_heads\n",
        "        Q = self.split_heads(Q, batch_size) # (batch, n_heads, seq_len_q, head_dim)\n",
        "        K = self.split_heads(K, batch_size) # (batch, n_heads, seq_len_k, head_dim)\n",
        "        V = self.split_heads(V, batch_size) # (batch, n_heads, seq_len_v, head_dim)\n",
        "\n",
        "        # 3. Apply scaled dot-product attention for each head\n",
        "        # The attention layer takes (batch, ..., seq_len, dim)\n",
        "        # Shapes are (batch, n_heads, seq_len, head_dim)\n",
        "        # The mask should be broadcastable, e.g., (batch, 1, 1, seq_len_k) or (batch, 1, seq_len_q, seq_len_k)\n",
        "        attention_output, attention_weights = self.attention(Q, K, V, mask=mask)\n",
        "        # attention_output shape: (batch, n_heads, seq_len_q, head_dim)\n",
        "        # attention_weights shape: (batch, n_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "        # 4. Concatenate heads and apply final linear layer\n",
        "        attention_output = self.combine_heads(attention_output, batch_size) # (batch, seq_len_q, embed_dim)\n",
        "        output = self.fc_out(attention_output)\n",
        "\n",
        "        # Apply dropout and Layer Normalization (common in Transformers)\n",
        "        # Typically includes a residual connection before LayerNorm:\n",
        "        # output = self.dropout(output)\n",
        "        # output = self.layer_norm(query_input + output) # Add & Norm\n",
        "        # For simplicity here, just showing the MHA core output.\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        # Usually only return the final output, but weights can be useful for analysis\n",
        "        # return output, attention_weights\n",
        "        return output\n",
        "\n",
        "# Conceptual Usage:\n",
        "# embed_dim = 512\n",
        "# num_heads = 8\n",
        "# batch_size = 4\n",
        "# seq_len = 10\n",
        "#\n",
        "# mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "# input_tensor = torch.randn(batch_size, seq_len, embed_dim)\n",
        "#\n",
        "# # Self-attention example\n",
        "# output = mha(input_tensor, input_tensor, input_tensor)\n",
        "# print(f\"Multi-Head Attention Output shape: {output.shape}\") # Should be (batch, seq_len, embed_dim)\n",
        "\n",
        "print(\"Implemented Multi-Head Attention structure.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# EXERCISE 4.1\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "e3Q5mtJeL0zR",
        "outputId": "05e80bf5-16e9-458f-fef7-f35556003d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading IMDB dataset with vocab size: 10000...\n",
            "Loaded 25000 training sequences and 25000 test sequences.\n",
            "Padding/truncating sequences to max length: 256...\n",
            "Padded training sequences shape: (25000, 256)\n",
            "Padded test sequences shape: (25000, 256)\n",
            "Building the model...\n",
            "\n",
            "Training the model for 1 epochs with batch size 64...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 890ms/step - accuracy: 0.6049 - loss: 0.6460 - val_accuracy: 0.7374 - val_loss: 0.5058\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SentimentAnalysisLSTM\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"SentimentAnalysisLSTM\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ WordEmbedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dropout_Emb (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ BiLSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dropout_LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ OutputClassifier (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ WordEmbedding (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │       \u001b[38;5;34m640,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dropout_Emb (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ BiLSTM (\u001b[38;5;33mBidirectional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m66,048\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dropout_LSTM (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ OutputClassifier (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,118,533</span> (8.08 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,118,533\u001b[0m (8.08 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">706,177</span> (2.69 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m706,177\u001b[0m (2.69 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,412,356</span> (5.39 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,412,356\u001b[0m (5.39 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating the model on the test set...\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 156ms/step - accuracy: 0.7272 - loss: 0.5132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.5090\n",
            "Test Accuracy: 0.7328\n",
            "\n",
            "Saving the trained model and training history...\n",
            "Model saved to sentiment_lstm_model.h5\n",
            "Training history saved to training_history.json\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# --- Text Classification using RNN (LSTM) ---\n",
        "#\n",
        "# Task Chosen: Text Classification\n",
        "#\n",
        "# 1. Problem Definition and Requirements:\n",
        "#    - Specific Problem: Sentiment Analysis of IMDB movie reviews. Classify reviews as either positive (1) or negative (0).\n",
        "#    - Input: A sequence of integers representing words in a movie review. Each integer is an index into a vocabulary.\n",
        "#    - Output: A single floating-point value between 0 and 1, representing the probability of the review being positive.\n",
        "#              A threshold (e.g., 0.5) can be used to convert this probability into a binary class label (0 or 1).\n",
        "#    - Evaluation Metric: Accuracy - the proportion of correctly classified reviews.\n",
        "#\n",
        "# 2. Dataset Choice:\n",
        "#    - IMDB Movie Review Dataset: Provided by Keras (`tf.keras.datasets.imdb`).\n",
        "#    - Justification: This is a standard benchmark dataset for binary sentiment classification. It contains 25,000 reviews for training\n",
        "#      and 25,000 for testing, already preprocessed into sequences of word indices. It's readily available and suitable\n",
        "#      for demonstrating sequence modeling techniques.\n",
        "\n",
        "# Reference: (https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/sentiment_analysis/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html)\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "# Parameters\n",
        "VOCAB_SIZE = 10000  # Number of words to keep from the dataset (most frequent)\n",
        "MAX_LEN = 256       # Maximum length of sequences (reviews)\n",
        "EMBEDDING_DIM = 64  # Dimension of word embeddings\n",
        "LSTM_UNITS = 64     # Number of units in the LSTM layer\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1\n",
        "\n",
        "# Load the IMDB dataset\n",
        "# - num_words=VOCAB_SIZE: Keeps only the top `VOCAB_SIZE` most frequent words.\n",
        "#   Less frequent words are replaced with a special out-of-vocabulary (OOV) token.\n",
        "#   This helps manage the vocabulary size and implicitly handles OOV words not seen\n",
        "#   during vocabulary creation by mapping them to the OOV index (index 2).\n",
        "print(f\"Loading IMDB dataset with vocab size: {VOCAB_SIZE}...\")\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
        "print(f\"Loaded {len(x_train)} training sequences and {len(x_test)} test sequences.\")\n",
        "\n",
        "# Handle variable-length sequences using padding\n",
        "# - Sequences (reviews) have different lengths.\n",
        "# - `pad_sequences` transforms a list of sequences into a 2D NumPy array.\n",
        "# - `maxlen=MAX_LEN`: Ensures all sequences have the same length (`MAX_LEN`).\n",
        "#   Sequences longer than `MAX_LEN` are truncated (from the beginning by default).\n",
        "#   Sequences shorter than `MAX_LEN` are padded with 0s (at the beginning by default).\n",
        "# - Padding is essential for feeding data into the neural network in fixed-size batches.\n",
        "# - The Embedding layer can be configured to ignore padding (mask_zero=True).\n",
        "print(f\"Padding/truncating sequences to max length: {MAX_LEN}...\")\n",
        "x_train_padded = pad_sequences(x_train, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "x_test_padded = pad_sequences(x_test, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "print(f\"Padded training sequences shape: {x_train_padded.shape}\")\n",
        "print(f\"Padded test sequences shape: {x_test_padded.shape}\")\n",
        "\n",
        "# --- Model Architecture Design ---\n",
        "print(\"Building the model...\")\n",
        "model = Sequential(name=\"SentimentAnalysisLSTM\")\n",
        "\n",
        "# 2. Design the complete architecture with justifications:\n",
        "\n",
        "# Input Representation (Embedding Strategy)\n",
        "# - `Embedding` layer: Maps the integer word indices into dense vectors of fixed size (`EMBEDDING_DIM`).\n",
        "# - `input_dim=VOCAB_SIZE`: The size of the vocabulary (number of unique words + padding + OOV).\n",
        "# - `output_dim=EMBEDDING_DIM`: The desired dimension of the word embeddings.\n",
        "# - `input_length=MAX_LEN`: The length of the input sequences.\n",
        "# - `mask_zero=True`: Tells subsequent layers to ignore timesteps where the input is 0 (the padding value).\n",
        "#   This is crucial for handling padded sequences correctly in RNNs.\n",
        "# - Justification: Embeddings capture semantic relationships between words. Training them as part of the model\n",
        "#   allows them to be tailored to the specific task (sentiment analysis).\n",
        "#   Using `mask_zero=True` correctly handles the variable sequence lengths introduced by padding.\n",
        "model.add(Embedding(input_dim=VOCAB_SIZE,\n",
        "                    output_dim=EMBEDDING_DIM,\n",
        "                    input_length=MAX_LEN,\n",
        "                    mask_zero=True,\n",
        "                    name=\"WordEmbedding\"))\n",
        "\n",
        "# Prevent Overfitting: Dropout after Embedding\n",
        "# - Dropout randomly sets a fraction of input units to 0 at each update during training.\n",
        "# - Helps prevent the model from relying too heavily on specific co-adaptations of neurons.\n",
        "# - Applied after the embedding layer to regularize the learned embeddings.\n",
        "model.add(Dropout(0.3, name=\"Dropout_Emb\"))\n",
        "\n",
        "# RNN/LSTM Layer Configuration\n",
        "# - `Bidirectional(LSTM(...))`: Processes the sequence in both forward and backward directions.\n",
        "# - `LSTM_UNITS`: The number of hidden units in the LSTM cell. This determines the layer's capacity.\n",
        "# - `return_sequences=False`: Only the output of the last timestep is needed for classification,\n",
        "#   so it doesn't return the full sequence of outputs from the LSTM.\n",
        "#   If stacking LSTMs was done, would set this to True for intermediate layers.\n",
        "# - Justification: Bidirectional LSTM allows the model to capture context from both past and future words\n",
        "#   in the review, which is often beneficial for understanding sentiment.\n",
        "#   LSTM is chosen over simple RNN to better handle potential vanishing gradient issues in longer sequences.\n",
        "#   A single layer with `LSTM_UNITS` is chosen as a reasonable starting point for this task.\n",
        "model.add(Bidirectional(LSTM(units=LSTM_UNITS, dropout=0.3, recurrent_dropout=0.3), name=\"BiLSTM\"))\n",
        "# dropout=0.3 applies dropout to the input connections of the LSTM cell.\n",
        "# recurrent_dropout=0.3 applies dropout to the recurrent connections.\n",
        "\n",
        "# Prevent Overfitting: Dropout before Output Layer\n",
        "# - Applied after the LSTM layer to regularize its output before feeding it to the final classifier.\n",
        "model.add(Dropout(0.3, name=\"Dropout_LSTM\"))\n",
        "\n",
        "# Output Layer Design\n",
        "# - `Dense` layer: A standard fully connected neural network layer.\n",
        "# - `units=1`: Outputs a single value (the probability of being positive).\n",
        "# - `activation='sigmoid'`: Sigmoid activation function squashes the output between 0 and 1,\n",
        "#   suitable for binary probability prediction.\n",
        "# - Justification: A single dense neuron with sigmoid activation is the standard way to produce\n",
        "#   a binary classification output.\n",
        "model.add(Dense(units=1, activation='sigmoid', name=\"OutputClassifier\"))\n",
        "\n",
        "# Loss Function Selection\n",
        "# - `loss='binary_crossentropy'`: Standard loss function for binary classification problems.\n",
        "#   It measures the difference between the predicted probabilities and the true binary labels (0 or 1).\n",
        "# - Justification: Binary cross-entropy is mathematically appropriate for optimizing models\n",
        "#   that output probabilities for two classes.\n",
        "\n",
        "# Optimizer Selection\n",
        "# - `optimizer='adam'`: An efficient and commonly used optimization algorithm.\n",
        "# - Justification: Adam adapts the learning rate for each parameter and generally works well\n",
        "#   for a wide range of problems, often converging faster than standard SGD.\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy']) # Evaluation metric\n",
        "\n",
        "\n",
        "# --- Model Training ---\n",
        "print(f\"\\nTraining the model for {EPOCHS} epochs with batch size {BATCH_SIZE}...\")\n",
        "\n",
        "# Callback for Early Stopping\n",
        "# - Monitors the validation loss (`val_loss`).\n",
        "# - `patience=3`: Stops training if `val_loss` doesn't improve for 3 consecutive epochs.\n",
        "# - `restore_best_weights=True`: Restores model weights from the epoch with the best `val_loss`.\n",
        "# - Justification: Early stopping is a crucial technique to prevent overfitting by stopping training\n",
        "#   when the model starts to perform worse on unseen validation data.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x_train_padded,\n",
        "                    y_train,\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_split=0.2, # Use 20% of training data for validation\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=1)\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# --- Model Evaluation ---\n",
        "print(\"\\nEvaluating the model on the test set...\")\n",
        "loss, accuracy = model.evaluate(x_test_padded, y_test, batch_size=BATCH_SIZE, verbose=1)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# --- Save Model and History ---\n",
        "print(\"\\nSaving the trained model and training history...\")\n",
        "\n",
        "# Save the entire model (architecture, weights, optimizer state)\n",
        "model_save_path = 'sentiment_lstm_model.h5'\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "# Save the training history (loss and accuracy metrics)\n",
        "history_save_path = 'training_history.json'\n",
        "# Convert history values to basic Python types for JSON serialization\n",
        "history_dict_serializable = {key: [float(val) for val in value] for key, value in history.history.items()}\n",
        "\n",
        "with open(history_save_path, 'w') as f:\n",
        "    json.dump(history_dict_serializable, f, indent=4)\n",
        "print(f\"Training history saved to {history_save_path}\")\n",
        "\n",
        "# --- Summary of Practical Challenges Addressed ---\n",
        "#\n",
        "# 3. Address practical challenges:\n",
        "#    - Variable-length sequences:\n",
        "#      - Handled by padding sequences to `MAX_LEN` using `pad_sequences`. (In Keras, the pad_sequences function is used to ensure that all sequences in a dataset have the same length by padding shorter sequences with zeros.)(https://medium.com/@prudhviraju.srivatsavaya/advantages-and-disadvantages-of-pad-sequences-keras-layer-4ea08a7eee5c)\n",
        "#      - The `Embedding` layer's `mask_zero=True` argument ensures that padded values (0s) are ignored # (https://www.tensorflow.org/guide/keras/understanding_masking_and_padding)\n",
        "#        by subsequent layers (like the LSTM), preventing them from influencing the results.\n",
        "#\n",
        "#    - Preventing overfitting:\n",
        "#      - Dropout: Applied after the Embedding layer and within/after the LSTM layer (`Dropout` layers and LSTM's `dropout`/`recurrent_dropout` args).\n",
        "#        This randomly deactivates neurons during training, forcing the network to learn more robust features. (https://www.restack.io/p/hyperparameter-tuning-answer-preventing-lstm-overfitting-cat-ai)\n",
        "#      - Early Stopping: Used via Keras Callbacks (`EarlyStopping`). Monitors validation loss and stops training\n",
        "#        when performance on the validation set stops improving, preventing the model from memorizing the training data. (https://www.geeksforgeeks.org/using-early-stopping-to-reduce-overfitting-in-neural-networks/)\n",
        "#      - Limited Vocabulary Size (`VOCAB_SIZE`): Reduces model complexity by focusing on the most frequent words. (https://aclanthology.org/2025.coling-industry.64.pdf) (https://shekhargulati.com/2024/12/11/why-llm-vocabulary-size-matters/#:~:text=Conclusion,for%20researchers%20and%20practitioners%20alike.)\n",
        "#      - Fixed Sequence Length (`MAX_LEN`): Limits the complexity the model needs to handle. (https://medium.com/@cerebras/context-is-everything-why-maximum-sequence-length-matters-for-ai-fa1f4c81009f)\n",
        "#\n",
        "#    - Out-of-vocabulary (OOV) words:\n",
        "#      - Handled implicitly by `imdb.load_data(num_words=VOCAB_SIZE)`. Words not in the top `VOCAB_SIZE`\n",
        "#        are mapped to a specific index (usually 2). The `Embedding` layer learns a representation for this OOV index\n",
        "#        just like any other word index. While not ideal (all unknown words map to the same vector), it's a\n",
        "#        baseline approach. More advanced techniques involve character-level embeddings or pre-trained\n",
        "#        embeddings like FastText that can generate vectors for OOV words. (https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data)\n",
        "#\n",
        "#    - Specific optimization challenges:\n",
        "#      - Vanishing/Exploding Gradients: LSTMs are specifically designed to mitigate the vanishing gradient problem compared to simple RNNs.\n",
        "#        For exploding gradients (less common with LSTMs but possible), techniques like gradient clipping could be added during\n",
        "#        optimizer configuration (`tf.keras.optimizers.Adam(clipvalue=1.0)`), although not explicitly implemented here. (https://www.baeldung.com/cs/lstm-vanishing-gradient-prevention#:~:text=Even%20though%20LSTMs%20are%20better,gradient%20explosion%20in%20certain%20cases.) (https://cnvrg.io/gradient-clipping/#elementor-toc__heading-anchor-9)\n",
        "#      - Optimizer Choice: Adam optimizer was chosen, which adapts learning rates per parameter and is generally robust. (https://www.analyticsvidhya.com/blog/2023/09/what-is-adam-optimizer/)\n",
        "#      - Convergence: Early stopping helps find a good stopping point. Learning rate scheduling could be added for finer control if needed. (https://d2l.ai/chapter_optimization/lr-scheduler.html)\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# EXERCISE 4.2\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6jf9KVoEeGp",
        "outputId": "b9ae8de6-6788-4abd-8d95-be399c049014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Embeddings Shape: torch.Size([9, 64])\n",
            "Context Vectors Shape: torch.Size([9, 32])\n",
            "Attention Weights Shape: torch.Size([9, 9])\n",
            "\n",
            "Sample Attention Weights (first word):\n",
            "tensor([0.1118, 0.1130, 0.0993, 0.1077, 0.1120, 0.1140, 0.1068, 0.1200, 0.1153],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "\n",
            "Attention Weights Matrix (rounded):\n",
            "tensor([[0.1100, 0.1100, 0.1000, 0.1100, 0.1100, 0.1100, 0.1100, 0.1200, 0.1200],\n",
            "        [0.1100, 0.1100, 0.1000, 0.1100, 0.1100, 0.1100, 0.1100, 0.1200, 0.1100],\n",
            "        [0.1200, 0.1100, 0.1000, 0.1100, 0.1100, 0.1200, 0.1000, 0.1200, 0.1100],\n",
            "        [0.1200, 0.1100, 0.1000, 0.1000, 0.1100, 0.1100, 0.1100, 0.1200, 0.1200],\n",
            "        [0.1200, 0.1100, 0.1000, 0.1100, 0.1100, 0.1100, 0.1000, 0.1200, 0.1200],\n",
            "        [0.1200, 0.1100, 0.1000, 0.1000, 0.1100, 0.1100, 0.1100, 0.1200, 0.1200],\n",
            "        [0.1100, 0.1100, 0.1000, 0.1000, 0.1100, 0.1100, 0.1100, 0.1200, 0.1100],\n",
            "        [0.1200, 0.1100, 0.1000, 0.1100, 0.1100, 0.1100, 0.1100, 0.1200, 0.1200],\n",
            "        [0.1200, 0.1100, 0.1000, 0.1100, 0.1100, 0.1100, 0.1000, 0.1200, 0.1200]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Reference: (https://mohdfaraaz.medium.com/implementing-self-attention-from-scratch-in-pytorch-776ef7b8f13e)\n",
        "class SimpleSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        super(SimpleSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        # Linear layers to project embeddings into Query, Key, and Value spaces\n",
        "        # In a real Transformer, these would be learned parameters.\n",
        "        self.query_layer = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "        self.key_layer = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "        self.value_layer = nn.Linear(embed_dim, head_dim, bias=False)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        # Input embeddings shape: (sequence_length, embed_dim)\n",
        "\n",
        "        # Step 1: Project embeddings into Query, Key, and Value vectors\n",
        "        # Q, K, V shape: (sequence_length, head_dim)\n",
        "        Q = self.query_layer(embeddings)\n",
        "        K = self.key_layer(embeddings)\n",
        "        V = self.value_layer(embeddings)\n",
        "\n",
        "        # Step 2: Compute attention scores (raw alignment scores)\n",
        "        # Calculate dot products between each Query vector and all Key vectors.\n",
        "        # Q shape: (sequence_length, head_dim)\n",
        "        # K.transpose(-2, -1) shape: (head_dim, sequence_length)\n",
        "        # attention_scores shape: (sequence_length, sequence_length)\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "\n",
        "        # Optional scaling (important in full Transformers to prevent large values)\n",
        "        # Scale by the square root of the key dimension (head_dim)\n",
        "        scale_factor = np.sqrt(self.head_dim)\n",
        "        attention_scores = attention_scores / scale_factor\n",
        "\n",
        "        # Step 3: Apply softmax to get attention weights\n",
        "        # Softmax is applied row-wise, ensuring weights for each query word sum to 1.\n",
        "        # attention_weights shape: (sequence_length, sequence_length)\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # Step 4: Compute the context vectors (weighted sum of Value vectors)\n",
        "        # Multiply the attention weights by the Value vectors.\n",
        "        # attention_weights shape: (sequence_length, sequence_length)\n",
        "        # V shape: (sequence_length, head_dim)\n",
        "        # context_vectors shape: (sequence_length, head_dim)\n",
        "        context_vectors = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return context_vectors, attention_weights\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Define parameters\n",
        "    embed_dim = 64  # Dimensionality of word embeddings\n",
        "    head_dim = 32   # Dimensionality for Q, K, V projections (single head)\n",
        "    seq_len = 9     # Sequence length of the example sentence\n",
        "\n",
        "    # Create dummy embeddings (replace with actual embeddings in a real model)\n",
        "    # Shape: (sequence_length, embed_dim)\n",
        "    dummy_embeddings = torch.rand(seq_len, embed_dim)\n",
        "\n",
        "    # Instantiate the attention mechanism\n",
        "    attention_mechanism = SimpleSelfAttention(embed_dim, head_dim)\n",
        "\n",
        "    # Compute attention\n",
        "    context_vectors, attention_weights = attention_mechanism(dummy_embeddings)\n",
        "\n",
        "    print(\"Input Embeddings Shape:\", dummy_embeddings.shape)\n",
        "    print(\"Context Vectors Shape:\", context_vectors.shape)\n",
        "    print(\"Attention Weights Shape:\", attention_weights.shape)\n",
        "    print(\"\\nSample Attention Weights (first word):\")\n",
        "    print(attention_weights[0])\n",
        "    print(\"\\nAttention Weights Matrix (rounded):\")\n",
        "    print(torch.round(attention_weights * 100) / 100) # Rounded for readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NL7wTz3tESke",
        "outputId": "364f583e-c13a-43ff-9091-be6139eeb1a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: 'The athlete who won the race trained for many years.'\n",
            "Tokens: ['The', 'athlete', 'who', 'won', 'the', 'race', 'trained', 'for', 'many', 'years']\n",
            "\n",
            "Generating demonstration attention heatmap (manually created weights)...\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 850x650 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Explanation ---\n",
            "This heatmap visualizes a manually created set of attention weights to demonstrate\n",
            "Each row corresponds to a 'query' word (the word doing the attending).\n",
            "Each column corresponds to a 'key' word (the word being attended to).\n",
            "The value in cell (row i, column j) represents how much attention word 'i' pays to word 'j'.\n",
            "Brighter colors indicate higher attention weights.\n",
            "\n",
            "Observed Patterns (Manually Created):\n",
            "- High attention from 'who' to 'athlete' (row 2, col 1) - Relative pronoun referring to subject.\n",
            "- High attention from 'won' to 'athlete' (row 3, col 1) - Verb attending to its subject.\n",
            "- High attention from 'won' to 'race' (row 3, col 5) - Verb attending to its object.\n",
            "- High attention from 'trained' to 'athlete' (row 6, col 1) - Long-distance subject connection.\n",
            "- High attention from 'many' to 'years' (row 8, col 9) - Adjective modifying noun.\n",
            "- Attention from 'for' to 'trained' and 'years' - Preposition relating parts of the sentence.\n",
            "- Relatively higher attention along the diagonal - Words attending to themselves.\n",
            "This heatmap doesn't reflect the output of the untrained SimpleSelfAttention model.\n",
            "It is for illustrating the desired outcome after successful training.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Import the SimpleSelfAttention class from the other file\n",
        "# ( Import it to show where it would be used, but manually create weights below --> to show Strong attention athelete and who for example )\n",
        "# from attention_mechanism_design import SimpleSelfAttention\n",
        "# Reference: https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention\n",
        "\n",
        "def visualize_attention(sentence, attention_weights, title=\"Self-Attention Weights Heatmap\"):\n",
        "    # Detach the tensor if it's still attached (safety measure)\n",
        "    if attention_weights.requires_grad:\n",
        "        attention_weights_np = attention_weights.detach().numpy()\n",
        "    else:\n",
        "        attention_weights_np = attention_weights.numpy()\n",
        "\n",
        "    # Create the heatmap\n",
        "    plt.figure(figsize=(8.5, 6.5)) # Slightly larger figure\n",
        "    sns.heatmap(attention_weights_np,\n",
        "                xticklabels=sentence,\n",
        "                yticklabels=sentence,\n",
        "                annot=True,  # Show the weight values on the heatmap\n",
        "                cmap=\"viridis\", # Color map choice\n",
        "                fmt=\".2f\",    # Format annotations to 2 decimal places\n",
        "                linewidths=.5, # Add lines between cells\n",
        "                linecolor='black',\n",
        "                annot_kws={\"size\": 9}) # Adjust annotation font size\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Key (Words being attended to)\")\n",
        "    plt.ylabel(\"Query (Words doing the attending)\")\n",
        "    plt.xticks(rotation=45, ha='right') # Improve x-axis label readability\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Define the Example Sentence\n",
        "    sentence_text = \"The athlete who won the race trained for many years.\"\n",
        "    # Simple tokenization by splitting on space\n",
        "    tokens = sentence_text.rstrip('.').split()\n",
        "    seq_len = len(tokens)\n",
        "    token_map = {token: i for i, token in enumerate(tokens)} # Helper map token -> index\n",
        "\n",
        "    # --- Manual Creation of Weights for Demonstration ---\n",
        "    # In a real applications, trained model, these weights would be calculated by the attention mechanism.\n",
        "    # Here, manually set to illustrate the expected patterns for the new sentence. That's also why randomization is not used.\n",
        "\n",
        "    # Initialize with low baseline weights, slightly higher on diagonal\n",
        "    manual_weights = torch.full((seq_len, seq_len), 0.03) # Even lower baseline for this sentence\n",
        "    manual_weights = manual_weights + torch.diag(torch.full((seq_len,), 0.08)) # Add to diagonal\n",
        "\n",
        "    # Inject the specific high-attention patterns for the new sentence:\n",
        "    # Strong attention between \"athlete\" and \"won\"\n",
        "    manual_weights[token_map['won'], token_map['athlete']] = 0.5\n",
        "    manual_weights[token_map['athlete'], token_map['won']] = 0.4\n",
        "\n",
        "    # Strong attention between \"who\" and \"athlete\"\n",
        "    manual_weights[token_map['who'], token_map['athlete']] = 0.6\n",
        "    manual_weights[token_map['athlete'], token_map['who']] = 0.3\n",
        "\n",
        "    # \"trained\" attends to \"athlete\" despite distance\n",
        "    manual_weights[token_map['trained'], token_map['athlete']] = 0.9\n",
        "\n",
        "    # Other meaningful connections\n",
        "    manual_weights[token_map['won'], token_map['race']] = 0.4\n",
        "    manual_weights[token_map['race'], token_map['won']] = 0.4\n",
        "    manual_weights[token_map['trained'], token_map['years']] = 0.3\n",
        "    manual_weights[token_map['many'], token_map['years']] = 0.5\n",
        "    manual_weights[token_map['for'], token_map['years']] = 0.4\n",
        "\n",
        "    # Normalize rows to sum roughly to 1 (like softmax would)\n",
        "    # This is a simplified normalization for demonstration purposes.\n",
        "    row_sums = manual_weights.sum(dim=1, keepdim=True)\n",
        "    # Avoid division by zero if a row happens to be all zeros\n",
        "    row_sums[row_sums == 0] = 1\n",
        "    attention_weights_demo = manual_weights / row_sums\n",
        "\n",
        "    # --- End of Manual Creation ---\n",
        "\n",
        "    # 5. Visualize the DEMONSTRATION Attention Weights\n",
        "    print(f\"Sentence: '{sentence_text}'\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(\"\\nGenerating demonstration attention heatmap (manually created weights)...\")\n",
        "    visualize_attention(tokens, attention_weights_demo, title=\"Demonstration of Expected Attention Patterns\")\n",
        "\n",
        "    # 6. Explanation of Visualization and Expected Patterns\n",
        "    print(\"\\n--- Explanation ---\")\n",
        "    print(\"This heatmap visualizes a manually created set of attention weights to demonstrate\")\n",
        "    print(\"Each row corresponds to a 'query' word (the word doing the attending).\")\n",
        "    print(\"Each column corresponds to a 'key' word (the word being attended to).\")\n",
        "    print(\"The value in cell (row i, column j) represents how much attention word 'i' pays to word 'j'.\")\n",
        "    print(\"Brighter colors indicate higher attention weights.\")\n",
        "    print(\"\\nObserved Patterns (Manually Created):\")\n",
        "    print(f\"- High attention from 'who' to 'athlete' (row {token_map['who']}, col {token_map['athlete']}) - Relative pronoun referring to subject.\")\n",
        "    print(f\"- High attention from 'won' to 'athlete' (row {token_map['won']}, col {token_map['athlete']}) - Verb attending to its subject.\")\n",
        "    print(f\"- High attention from 'won' to 'race' (row {token_map['won']}, col {token_map['race']}) - Verb attending to its object.\")\n",
        "    print(f\"- High attention from 'trained' to 'athlete' (row {token_map['trained']}, col {token_map['athlete']}) - Long-distance subject connection.\")\n",
        "    # Even though \"trained\" and \"athlete\" are far apart in the sequence, the attention heatmap shows that \"trained\" strongly attends to \"athlete\"\n",
        "    # (with a normalized weight of ~0.59), meaning that the model is still able to capture the semantic connection across this distance. In RNNs,\n",
        "    # learning long-range dependencies like that is hard, they struggle with vanishing gradients. But self-attention mechanisms (like in Transformers)\n",
        "    # don’t care about token distance, any token can directly attend to any other token, regardless of position. This is one of the biggest strengths of attention,\n",
        "    # it makes long-distance relationships easy to learn.\n",
        "    print(f\"- High attention from 'many' to 'years' (row {token_map['many']}, col {token_map['years']}) - Adjective modifying noun.\")\n",
        "    print(f\"- Attention from 'for' to 'trained' and 'years' - Preposition relating parts of the sentence.\")\n",
        "    print(\"- Relatively higher attention along the diagonal - Words attending to themselves.\")\n",
        "    print(\"This heatmap doesn't reflect the output of the untrained SimpleSelfAttention model.\")\n",
        "    print(\"It is for illustrating the desired outcome after successful training.\")\n",
        "\n",
        "    # --- Visualize the UNTRAINED weights as well ---\n",
        "    # If want to see the random-like patterns from the actual untrained model again:\n",
        "    # print(\"\\nGenerating attention heatmap from UNTRAINED model...\")\n",
        "    # embed_dim = 64\n",
        "    # head_dim = 32\n",
        "    # torch.manual_seed(42)    # Fix randomness for reproducibility\n",
        "    # dummy_embeddings = torch.rand(seq_len, embed_dim)      # Creates random input token embeddings (pretend tokens, like gibberish)\n",
        "    # attention_mechanism = SimpleSelfAttention(embed_dim, head_dim)     # Custom self-attention layer, initialized with random weights\n",
        "    # attention_mechanism.eval()         # Set to evaluation mode (no dropout, etc.)\n",
        "    # with torch.no_grad():\n",
        "    #      _, attention_weights_untrained = attention_mechanism(dummy_embeddings)          # Get the attention scores from the untrained model\n",
        "    # visualize_attention(tokens, attention_weights_untrained, title=\"Actual Attention Weights (Untrained Model)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
